{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza.resources.common\n",
    "from stanza.utils.conll import CoNLL\n",
    "from textcomplexity.cli import surface_based, sentence_based, pos_based, dependency_based, read_language_definition\n",
    "from textcomplexity.utils import conllu\n",
    "\n",
    "LANGUAGE = 'en'\n",
    "PRESET = 'all'\n",
    "# dataset = 'training'\n",
    "# dataset = 'valid'\n",
    "dataset = 'test'\n",
    "CSV_URL = f'../data/{dataset}_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_URL)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df.sample(frac=1, random_state=9).reset_index(drop=True)\n",
    "df_encoded = pd.DataFrame(df[['ID', 'Sentence EN']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stanza_resources_path = os.path.join(stanza.resources.common.DEFAULT_MODEL_DIR, \"resources.json\")\n",
    "if not os.path.isfile(stanza_resources_path):\n",
    "    stanza.resources.common.download_resources_json()\n",
    "stanza.download(LANGUAGE)\n",
    "stanza_pipeline = stanza.Pipeline(LANGUAGE, processors=\"tokenize,mwt,pos,lemma,depparse\")\n",
    "\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    doc = stanza_pipeline(sentence)\n",
    "    dicts = doc.to_dict()\n",
    "    conll = CoNLL.convert_dict(dicts)\n",
    "    sentence_encoded = []\n",
    "    for s in conll:\n",
    "        sentence_encoded.extend(['\\t'.join(token) for token in s] + [''])\n",
    "    return sentence_encoded\n",
    "\n",
    "\n",
    "df_encoded['sentence_encoded'] = df_encoded['Sentence EN'].apply(encode_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_features(sentence_encoded):\n",
    "    language, punct_tags, name_tags, open_tags, reference_frequency_list = read_language_definition(\n",
    "        f'resources/{LANGUAGE}.json')\n",
    "    all_results = {}\n",
    "    sentences, graphs = zip(*conllu.read_conllu_sentences(sentence_encoded * 2))\n",
    "    tokens = list(itertools.chain.from_iterable(sentences))\n",
    "    window_size = int(sentence_encoded[-2].split('\\t')[0])\n",
    "    results = []\n",
    "    try:\n",
    "        results.extend(surface_based(tokens, window_size, PRESET))\n",
    "    except:\n",
    "        print(f'Warning: surface_based failed. Features are set to NaN.')\n",
    "    results.extend(pos_based(tokens, punct_tags, name_tags, open_tags, reference_frequency_list, PRESET))\n",
    "    results.extend(sentence_based(sentences, punct_tags, PRESET))\n",
    "    results.extend(dependency_based(graphs, PRESET))\n",
    "    for r in results:\n",
    "        all_results[r.name] = {'value': r.value}\n",
    "        if r.stdev is not None:\n",
    "            all_results[r.name]['stdev'] = r.stdev\n",
    "        if r.length is not None:\n",
    "            all_results[r.name]['length'] = r.length\n",
    "            all_results[r.name]['length stdev'] = r.length_stdev\n",
    "    return all_results\n",
    "\n",
    "\n",
    "features = df_encoded['sentence_encoded'].apply(extract_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "for idx, row in features.items():\n",
    "    for k, v in row.items():\n",
    "        for kk, vv in v.items():\n",
    "            if k + \"_\" + kk not in feature_names:\n",
    "                feature_names.append(k + \"_\" + kk)\n",
    "features_dict = {name: np.full(len(df), np.nan) for name in feature_names}\n",
    "for idx, row in features.items():\n",
    "    for k, v in row.items():\n",
    "        for kk, vv in v.items():\n",
    "            features_dict[k + \"_\" + kk][idx] = vv\n",
    "\n",
    "df_features = pd.DataFrame.from_dict(features_dict)\n",
    "df_features = df_encoded.join(df_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_features.to_csv(f'../data/features/features_{dataset}_complexity_en.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}